# Classic AI Paper

<br><b>Binary arithmetic</b>
<br>Godefroy-Guillaume Leibnitz. Explication de l‚Äôarithm√©tique binaire, qui se sert des seuls caract√®res O et I avec des remarques sur son utilit√© et sur ce qu‚Äôelle donne le sens des anciennes figures
chinoises de Fohy (1703)

<br><b>Boolean Algebra</b>
<br>George Boole. The Laws of Thought (1854)

<br><b>Entropy</b>
<br>Ludwig Boltzmann. On the Relationship between the Second Fundamental Theorem of the Mechanical Theory of Heat and Probability Calculations Regarding the Conditions for Thermal Equilibrium (1877)
<br>Claude E. Shannon. A Mathematical Theory of Communication (1948)

<br><b>Anomaly Detection</b>
<br>K. Pearson. On lines and planes of closest fit to systems of points in space (Philosophical Magazine 1901)

<br><b>Logic Gate</b>
<br>Claude E. Shannon. A Symbolic Analysis of Relay and Switching Circuits (1937)

<br><b>McCulloch & Pitss Model</b>
<br>Warren McCulloch and Walter Pitss et al. A Logical Calculus of The Ideas Immanent in Nervous Activity (1943)

<br><b>Gradient Descent (GD)</b>
<br>C. Lemarechal. Cauchy and the Gradient Method. Doc Math Extra, pp. 251-254. (2012) 

<br><b>Von Neumann Architecture</b>
<br>J Von Neumann. First Draft of a Report on EDVAC (1945)

<br><b>Turing Machine</b>
<br>A. M. Turing. Intelligent Machinery (1948) 

<br><b>Turing Test</b>
<br>A. M. Turing. Computing Machinery and Intelligence (1950) 

<br><b>Artificial Intelligence</b>
<br>John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence (1955)

<br><b>Perceptron</b>
<br>Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain (Psychological Review 1958)

<br><b>LISP (List Processing)</b>
<br>By John McCarthy (1958)

<br><b>Alpha-Beta Pruning</b>
<br>Arthur L. Samuel. Some studies in mahine learning using the game of checkers (1959)

<br><b>Decision Tree</b>
<br>Morgan, J.N. & Sonquist, J.A. Problems in the analysis of survey data, and a proposal. (1963)

<br><b>Iris</b>
<br>R.A. Fisher' et al.  The Use of Multiple Measurements in Taxonomic Problems (1963)

<br><b>Automata</b>
<br>J Von Neumann, AW Burks. Theory of self-reproducing automata. (1966)
<br>Bingbin Liu. Transformers Learn Shortcuts to Automata. (ICLR 2023)

<br><b>K-Nearest Neighbors (K-NN)</b>
<br>T. M. COVER. Nearest Neighbor Pattern Classification (1967)

<br><b>Symbolic AI</b>
<br>Newell, J. C. Shaw, Allen Simon. Empirical explorations of the logic theory machine: a case study in heuristic (1957)
<br>Newell, Allen Simon and Herbert A. Human problem solving (1972)
<br>Newell, Allen Simon. Computer science as empirical inquiry: symbols and search (1976)

<br><b>Rescorla‚ÄìWagner Model</b>
<br>Rescorla, R.A. & Wagner, A.R. A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement (1972)

<br><b>Emergent Ability</b>
<br>P. W. Anderson et al. More Is Different (1972)
<br>Rylan Schaeffer et al. Are Emergent Abilities of Large Language Models a Mirage? (2023)

<br><b>Eligibility Traces</b>
<br>A. Klopf. Brain Function and Adaptive Systems: A Heterostatic Theory (1972)
<br>Satinder P.Singh & Rechard S.Sutton. Reinforcement learning with replacing eligibility traces (1996)

<br><b>Beam Search</b>
<br>B. T. Lowerre. The harpy speech recognition system. Carnegie Mellon University. (1976)
<br>PENG SI OW et al. Filtered beam search in scheduling. (1986)

<br><b>EM Algorithm</b>
<br>A. P. Dempster et al. Maximum Likelihood from Incomplete Data via the EM Algorithm (Journal of the Royal Statistical Society 1977)

<br><b>Bayesian Optimization</b>
<br>J Mockus et al. The application of Bayesian methods for seeking the extremum (1978)
<br>Jasper Snoek et al. Practical Bayesian Optimization of Machine Learning Algorithms (NeurIPS 2012)

<br><b>Constraint Satisfaction Problem</b>
<br>Geoffrey E Hinton. Using Relaxation to Find a Puppet. (1979)

<br><b>Outliers Detection</b>
<br>D. Hawkins. Identification of Outliers (1980)

<br><b>Temporal Difference Learning</b>
<br>Sutton, Richard S. Barto, Andrew G. Toward a modern theory of adaptive networks (Psychological Review 1981)

<br><b>Shallow Learning(Least Squares)</b>
<br>Stephen M. Stigler. Gauss and the Invention of Least Squares. (1981) 

<br><b>Neuroscience</b>
<br>David Hubel, Torsten Wiesel. Receptive fields of single neurons in the cat‚Äôs striate cortex (1959) Lawrence Roberts. Machine perception of three-dimensional solids (1963)
<br>David Mar. Vision: A computational investigation into the human representation and processing of visual information(1982)

<br><b>Neocognitron</b>
<br>Kunihiko Fukushima. Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (1980)

<br><b>K-means Clustering</b>
<br>STUART P. LLOYD. Least square quantization in PCM (1982)

<br><b>Hopfield Network</b>
<br>J J Hopfield. Neural networks and physical systems with emergent collective computational abilities (1982)

<br><b>CART</b>
<br>Breiman, L et al. Classzfication and Regression Trees. (Belmont, CA: Wadsworth International Group 1984)

<br><b>Boltzmann Machines</b>
<br>Geoffrey E. Hinton et al. A Learning Algorithm for Boltzmann Machines (1985)

<br><b>Distributed representations</b>
<br>Geoffrey E. Hinton et al.  Distributed representations (1986) 

<br><b>Iterative Dichotomiser 3</b>
<br>Quinlan, R. Induction of decision trees. (Machane Learnzng 1986)

<br><b>Backpropagation</b>
<br>H. J. Kelley. Gradient Theory of Optimal Flight Paths. ARS Journal, Vol. 30, No. 10, pp. 947-954. (1960)
<br>David E. Rumelhart et al. Learning representations by back-propagating errors (1986)

<br><b>Katz's back-off model</b>
<br>Katz, S. M. Estimation of probabilities from sparse data for the language model component of a speech recognizer (1987)

<br><b>Hidden Markov Models</b>
<br>Rabiner, L. A. Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.(Proceedings of the IEEE 1989)

<br><b>MARS(Multivariate Adaptive Regression Splines)</b>
<br>Friedman, J. H. Multivariate adaptive regression splines. (The Annals of Statzstzcs 1991)

<br><b>Dyslexi</b>
<br>Geoffrey E. Hinton et al.  Lesioning an attractor network: Investigations of acquired dyslexi (1991) 

<br><b>Mixture of Experts </b>
<br>Robert A. Jacobs et al. Adaptive Mixtures of Local Experts (MIT Press 1991)
<br>Noam Shazeer et al. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (ICLR 2017)
<br>Dmitry Lepikhin et al. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (ICLR 2021)
<br>William Fedus et al. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (JMLR 2022)
<br>Barret Zoph et al. ST-MoE: Designing Stable and Transferable Sparse Expert Models (2022)
<br>Trevor Gale et al. MegaBlocks: Efficient Sparse Training with Mixture-of-Experts (2022)
<br>Sheng Shen et al. Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models (ICLR 2024)


<br>Albert Q. Jiang et al. Mixture of Experts (2024)

<br><b>Object Recognition</b>
<br>David Lowe. Object Recognition from Local Scale-Invariant Features. (1992) 

<br><b>Singular Value Decomposition</b>
<br>G. W. Stewart. Early History of the Singular Value Decomposition (1993)

<br><b>Penn Treebank</b>
<br>Mitchell P. Marcus et al. Building a Large Annotated Corpus of English: The Penn Treebank (1993)

<br><b>Word Co-occurrence Probabilities</b>
<br>Ido Dagan et al. Similarity-Based Estimation of Word Cooccurrence Probabilities (ACL 1994)

<br><b>Maximum Entropy</b>
<br>Adwait R. A Maximum Entropy Model for POS tagging (1994)

<br><b>Complementary priors</b>
<br>Geoffrey E. Hinton et al. A fast learning algorithm for deep belief nets (1994)

<br><b>Kneser-Ney Smoothing</b>
<br>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling (ICASSP 1995)

<br><b>Speech Recognization</b>
<br>LeCun, Yann et al. Convolutional networks for images, speech, and time series. (The handbook of brain theory and neural networks 1995)

<br><b>BM25</b>
<br>Stephen Robertson et al. Okapi at TREC-3. In Overview of the Third Text REtrieval Conference(TREC-3). pages 109‚Äì126. (1995)

<br><b>SVM(Support Vector Machine)</b>
<br>Corinna Cortes, Vladimir Vapnik. Support-vector networks (1995)

<br><b>Statistical Machine Learning</b>
<br>Vladimir Vapnik. The Nature of Statistical Learning Theory (1995)

<br><b>NER(Named-Entity Recognition)</b>
<br>Lance Ramshaw, Mitch Marcus. Text Chunking using Transformation-Based Learning (VLC-WS 1995)

<br><b>TF-IDF</b>
<br>Thorsten Joachims. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization (1996)

<br><b>LeNet</b>
<br>Yann LeCun et al. GradientBased Learning Applied to Document Recognition (IEEE 1998)

<br><b>MNIST</b>
<br>LeCun et al. Gradient-based learning applied to document recognition (IEEE 1998)

<br><b>MEMM</b>
<br>McCallum et al. Maximum Entropy Markov Models for Information Extraction and Segmentation (ICML 2000)

<br><b>CRFs</b>
<br>J. Lafferty et al. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling SequenceData (ICML 2001)

<br><b>DBSCAN</b>
<br>Martin Ester et al. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise (KDD 1996)

<br><b>Adaboost</b>
<br>Yoav Freund, Robert E. Schapire. Experiments with a New Boosting Algorithm (1996)

<br><b>Graph Neural Network</b>
<br>Alessandro Sperduti et al. Supervised neural networks for the classification of structures (1997)

<br><b>LeNet</b>
<br>Yann LeCun Leon Bottou Yoshua Bengio, Patrick Haner. Gradient-Based Learning Applied to Document Recognition (1998)

<br><b>DNN</b>
<br>Yann LeCun Leon Bottou Yoshua Bengio, Patrick Haner. Gradient-Based Learning Applied to Document Recognition (1998)

<br><b>RNN</b>
<br>Rumelhart, David E; Hinton, Geoffrey E, and Williams, Ronald J. Learning internal representations by error propagation (Sept. 1985)
<br>Jordan, Michael I. Serial order: a parallel distributed processing approach (1986)

<br><b>DENDRAL</b>
<br>Edward A. Feigenbaum, Bruce G. Buchanan. DENDRAL and Meta-DENDRAL roots of knowledge systems and expert system applications (1993)

<br><b>Variational AutoEncoder(VAE)</b>
<br>Geoffrey E Hinton et al. Autoencoders, minimum description length, and helmholtz free energy (NeurIPS 1994)
<br>Diederik P Kingma, Max Welling. Auto-Encoding Variational Bayes (2013)

<br><b>LSTM</b>
<br>S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. (1995)

<br><b>EQP(Equation Prover)</b>
<br>William Mccune. Deep Blue. (1997)

<br><b>Random Forest</b>
<br>Leo Breiman. Random Forests. Machine Learning, Volume 45, pages 5‚Äì32. (2001)

<br><b>Deap Blue</b>
<br>M Campbell. Deep Blue. (2002)

<br><b>NPLM(Neural Probabilistic Language Model)</b>
<br>Yoshua Bengio et al. A neural probabilistic language model (Journal of Machine Learning Research 2003)

<br><b>LDA(Latent Dirichlet Allocation)</b>
<br>David M. Blei et al. A neural probabilistic language model (Journal of Machine Learning Research 2003)

<br><b>CoNLL-2003</b>
<br>Sang et al. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition (NAACL 2003)

<br><b>DRIVE (Digital Retinal Images for Vessel Extraction)</b>
<br>Joes Staal et al. Ridge-based vessel segmentation in color images of the retina (IEEE 2004)

<br><b>Feature</b>
<br>David G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints (IJCV 2004)
<br>Navneet Dalal, Bill Triggs. Histograms of Oriented Gradients for Human Detection (CVPR 2005)

<br><b>Reconstruction</b>
<br>Noah Snavely, Steven M. Seitz, Richard Szeliski. Photo Tourism: Exploring Photo Collections in 3D (ACM 2006)

<br><b>Connectionist Temporal Classification (CTC)</b>
<br>Alex Graves et al. Connectionist Temporal Classification, Labelling Unsegmented Sequence Data with RNN (ICML 2006)

<br><b>Deep Belief Network (DBN)</b>
<br>Geoffrey E. Hinton et al. A fast learning algorithm for deep belief nets (2006)
<br>Lee Honglak et al. Sparse deep belief net model for visual area V2 (NeurlIPS 2007)
<br>Hinton, Geoffrey E et al. Reducing the dimensionality of data with neural networks (Science 2006)
<br>Hinton, Geoffrey E et al. Training products of experts by minimizing contrastive divergence (Neural computation 2002)

<br><b>Autoencoder</b>
<br>Reducing the dimensionality of data with neural networks (2006)

<br><b>Greedy layer-wise training </b>
<br>Bengio, Yoshua, et al. "Greedy layer-wise training of deep networks. (NeurlIPS 2007)

<br><b>SLAM</b>
<br>Davison et al. MonoSLAM: Real-Time Single Camera SLAM (TPAMI 2007)

<br><b>Knowledge Graph</b>
<br>Fabian M. Suchanek et al. YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia (WWW 2007)

<br><b>t-SNE</b>
<br>Laurens van der Maaten et al. Visualizing Data using t-SNE (JMLR 2008)

<br><b>Denoising Autoencoder</b>
<br>Pascal Vincent  et al. Extracting and Composing Robust Features with Denoising Autoencoders (ICML 2008)

<br><b>The Four-Color Theorem</b>
<br>Georges Gonthier. Formal Proof‚ÄîThe Four- Color Theorem (2008)

<br><b>IEMOCAP (The Interactive Emotional Dyadic Motion Capture (IEMOCAP) Database)</b>
<br>Carlos Busso et al. IEMOCAP: interactive emotional dyadic motion capture database (2008)

<br><b>Deformable Part Model</b>
<br>Felzenszwalb, David McAllester, Deva Ramanan. A discriminatively trained, multiscale, deformable part model. (2008)

<br><b>Pubmed</b>
<br>Prithviraj Sen et al. Collective Classification in Network Data (AAAI 2008)

<br><b>t-SNE</b>
<br>Laurens van der Maaten et al. Visualizing Data using t-SNE (JMLR 2008)

<br><b>Relation Extraction</b>
<br>Mintz et al. Distant supervision for relation extraction without labeled data (ACL | IJCNLP 2009)

<br><b>ImageNet</b>
<br>Jia Deng et al. ImageNet: A Large-Scale Hierarchical Image Database (CVPR 2009)

<br><b>Domain Adaption</b>
<br>Shai Ben-David et al. A theory of learning from different domains (Mach Learn 2010)

<br><b>ReLU</b>
<br>Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines (ICML 2010)

<br><b>PASCAL VOC</b>
<br>Mark Everingham et al. The PASCAL Visual Object Classes (VOC) Challenge (IJCV 2010)

<br><b>Fold It</b>
<br>Seth Cooper et al. Predicting protein structures with a multiplayer online game (Nature 2010)

<br><b>Graphical Models</b>
<br>Sebastian Nowozin and Christoph H. Lampert. Structured Learning and Prediction in Computer Vision (2011)

<br><b>CUB-200-2011 (Caltech-UCSD Birds-200-2011)</b>
<br>Wah et al. The Caltech-UCSD Birds-200-2011 Dataset (2011)

<br><b>HMDB51</b>
<br>Hildegard Kuehne et al. HMDB: A large video database for human motion recognition (IEEE 2011)

<br><b>SVHN (Street View House Numbers)</b>
<br>Netzer et al. Reading digits in natural images with unsupervised feature learning (2011)

<br><b>Sicikit learn</b>
<br>Fabian Pedregosa et al. Scikit-learn: Machine Learning in Python (2011)
<br>Lars Buitinck et al. API design for machine learning software: experiences from the scikit-learn project (2013)

<br><b>Numpy</b>
<br>Stefan Van Der Walt et al. The NumPy array: a structure for efficient numerical computation (2011)
<br>Charles R. Harris et al. Array Programming with NumPy (2020)

<br><b>IMuJoCo</b>
<br>Emanuel Todorov et al. MuJoCo: A physics engine for model-based control (IEEE/RSJ IROS 2012)

<br><b>CIFAR</b>
<br>Alex Krizhevsky et al. Learning Multiple Layers of Features from Tiny Images (2012)

<br><b>NYUv2 (NYU-Depth V2)</b>
<br>Nathan Silberman et al. Indoor Segmentation and Support Inference from RGBD Images (LNIP 2012)

<br><b>KITTI-360</b>
<br>KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D (PAMI 2012)

<br><b>UCF101 (UCF101 Human Actions dataset)</b>
<br>Soomro et al. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild (2012)

<br><b>KITTI </b>
<br>Andreas Geiger et al. Are we ready for autonomous driving? The KITTI vision benchmark suite (IEEE 2012)

<br><b>LIDC-IDRI</b>
<br>Armato et al. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans (2011)

<br><b>Random Search</b>
<br>J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization (2012) 

<br><b>CNN(Alexnet)</b>
<br>A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks (2012)
<br>Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks (ECCV 2014)

<br><b>SST (Stanford Sentiment Treebank)</b>
<br>Socher et al. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank (EMNLP 2013)

<br><b>Human3.6M</b>
<br>Ionescu et al. Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments (IEEE 2013)

<br><b>ConvGNN</b>
<br>Joan Bruna et al. Spectral Networks and Locally Connected Networks on Graphs (2013)

<br><b>R-CNN</b>
<br>Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation (2013)

<br><b>Word2Vec</b>
<br>T. Mikolov et al. Efficient estimation of word representations in vector space (2013)

<br><b>Anomaly Detection</b>
<br>Charu C Aggarwal. An introduction to outlier analysis (2013)

<br><b>Dropout</b>
<br>N. Srivastava et al. Dropout: A simple way to prevent neural networks from overfitting (2014)

<br><b>Word Representation</b>
<br>Omer Levy et al. Neural Word Embedding as Implicit Matrix Factorization (2014)

<br><b>Adam</b>
<br>D. Kingma and J. Ba. Adam: A method for stochastic optimization (2014) 

<br><b>COCO (Microsoft Common Objects in Context)</b>
<br>Tsung-Yi Lin et al. Microsoft COCO: Common Objects in Context (ECCV 2014)

<br><b>Caffe</b>
<br>Yangqing Jia et al. Caffe: Convolutional Architecture for Fast Feature Embedding (ACM 2014)

<br><b>GRU(Gated Recurrent Unit)</b>
<br>Kyunghyun Cho et al. Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation (EMNLP 2014)

<br><b>PASCAL3D+</b>
<br>Yu Xiang et al. Beyond PASCAL: A benchmark for 3D object detection in the wild (IEEE 2014)

<br><b>DeCAF</b>
<br>Boris van Breugel, Trent Kyono, Jeroen Berrevoets, Mihaela van der Schaar. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition (ICML 2014)

<br><b>GAN</b>
<br>I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio. Generative adversarial nets (2014)

<br><b>FCN</b>
<br>Jonathan Long, Evan Shelhamer, Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. (2014)

<br><b>DeepFace</b>
<br>Y. Taigman et al.DeepFace: Closing the gap to human-level performance in face verification (2014)

<br><b>Seq2Seq</b>
<br>I. Sutskever et al. Sequence to sequence learning with neural networks. (2014)

<br><b>DQN (Deep Q-Network)</b>
<br>John Schulman et al. Playing Atari with Deep Reinforcement Learning (NeurIPS 2014)
<br>Volodymyr Mnih. Human level control through deep reinforcement learning (Nature 2015)

<br><b>Robotics: OpenAI Gym</b>
<br>Matthias Plappert et al. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research (NeurIPS 2014)

<br><b>GloVe</b>
<br>Jeffrey Pennington et al. GloVe: Global Vectors for Word Representation (EMNLP 2014)

<br><b>Text Classification with CNN</b>
<br>Yoon Kim. Convolutional Neural Networks for Sentence Classification (EMNLP 2014)

<br><b>CAM(Class-Activation Map)</b>
<br>Maxime Oquab et al. Is Object Localization for Free? - Weakly-Supervised Learning With Convolutional Neural Networks (CVPR 2015)

<br><b>Unsupervised Domain Adaptation</b>
<br>Yaroslav Ganin et al. Unsupervised Domain Adaptation by Backpropagation (2015)

<br><b>ResNet</b>
<br>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. (2015)

<br><b>Batch Normalization</b>
<br>S. Loffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. (2015)

<br><b>YOLO</b>
<br>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. You Only Look Once: Unified, Real-Time Object Detection (2015)

<br><b>ArcFace</b>
<br>Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep
Face Recognition (CVPR 2015)

<br><b>SUN RGB-D</b>
<br>Song et al. SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite (IEEE 2015)

<br><b>MovieLens</b>
<br>F. Maxwell Harper et al. The MovieLens Datasets: History and Context (ACL 2015)

<br><b>ModelNet</b>
<br>Wu et al. 3D ShapeNets: A Deep Representation for Volumetric Shapes (CVPR 2015)

<br><b>LibriSpeech</b>
<br>Vassil Panayotov et al. Librispeech: An ASR corpus based on public domain audio books (IEEE 2015)

<br><b>SNLI (Stanford Natural Language Inference)</b>
<br>Bowman et al. A large annotated corpus for learning natural language inference (EMNLP 2015)

<br><b>Visual Question Answering (VQA)</b>
<br>Agrawal et al. VQA: Visual Question Answering (ICCV 2015)

<br><b>ShapeNet</b>
<br>Chang et al. ShapeNet: An Information-Rich 3D Model Repository (2015)

<br><b>Model Compression</b>
<br>Cristian BucilÀòa et al. Model Compression (ACM SIGKDD 2006)
<br>O. Vinyals, J. A. Dean, G. E. Hinton. Distilling the Knowledge in a Neural Network. (2015)

<br><b>CelebA (CelebFaces Attributes Dataset)</b>
<br>Liu et al. Deep Learning Face Attributes in the Wild (IEEE 2015)

<br><b>ActivityNet</b>
<br>Heilbron et al. ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding (IEEE 2015)

<br><b>IModelNet</b>
<br>Wu et al. 3D ShapeNets: A Deep Representation for Volumetric Shapes (CVPR 2015)

<br><b>Deep learning</b>
<br>Yann LeCun, Yoshua Bengio, Geoffrey Hinton. Deep learning (NatureDeepReview 2015)

<br><b>TRPO</b>
<br>Schulman, John et al. Trust Region Policy Optimization. (2015)

<br><b>A3C (Asynchronous Advantage Actor Critic)</b>
<br>Volodymyr Mni et al. Asynchronous Methods for Deep Reinforcement Learning (ICML 2016)

<br><b>MCTS</b>
<br>Silver, D et al. Mastering the game of Go with deep neural networks and tree search (Nature 2016)

<br><b>DeepLab</b>
<br>Liang-Chieh Chen et al. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs (TPAMI 2016)

<br><b>R-FCN</b>
<br>Jifeng Dai et al. R-FCN: Object Detection via Region-based Fully Convolutional Networks (2016)

<br><b>LIME(Local Interpretable Model-agnostic Explanations)</b>
<br>Marco Tulio Ribeiro et al. Why Should I Trust You?": Explaining the Predictions of Any Classifier (NAACL 2016)

<br><b>Subword Model</b>
<br>Rico Sennrich et al. Neural Machine Translation of Rare Words with Subword Units (ACL 2016)

<br><b>Monte Carlo Dropout</b>
<br>Yarin Gal et al. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (ICML 2016)

<br><b>Graph Autoencoders</b>
<br>Thomas N. Kipf et al. Variational Graph Auto-Encoders (NeurIPS 2016)

<br><b>Document Classification</b>
<br>Z Yang et al. Hierarchical Attention Networks for Document Classification (NAACL 2016)

<br><b>Visual Intelligence</b>
<br>Brenden M. Lake et al. Building Machines That Learn and Think Like People (NeurIPS 2016)

<br><b>Imini-Imagenet</b>
<br>Vinyals et al. Matching Networks for One Shot Learning (NeurIPS 2016)

<br><b>IoU Loss</b>
<br>Jiahui Yu et al. UnitBox: An Advanced Object Detection Network (ACM MM 2016)

<br><b>XGBoost</b>
<br>Tianqi Chen et al. XGBoost: A Scalable Tree Boosting System (KDD 2016)

<br><b>DAVIS (Densely Annotated VIdeo Segmentation)</b>
<br>Perazzi et al. A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation (IEEE 2016)

<br><b>S3DIS (Stanford 3D Indoor Scene Dataset (S3DIS))</b>
<br>Armeni et al. 3D Semantic Parsing of Large-Scale Indoor Spaces (IEEE 2016)

<br><b>Universal Dependencies</b>
<br>Nivre et al. Universal Dependencies v1: A Multilingual Treebank Collection (LREC 2016)

<br><b>CheXpert</b>
<br>Irvin et al. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison (AAAI 2016)

<br><b>VCTK (CSTR VCTK Corpus)</b>
<br>Veaux et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (2016)

<br><b>MIMIC-III (The Medical Information Mart for Intensive Care III)</b>
<br>Johnson et al. MIMIC-III, a freely accessible critical care database (2016)

<br><b>SQuAD (Stanford Question Answering Dataset)</b>
<br>Rajpurkar et al. SQuAD: 100,000+ Questions for Machine Comprehension of Text (EMNLP 2016)

<br><b>Cityscapes</b>
<br>Cordts et al. The Cityscapes Dataset for Semantic Urban Scene Understanding  (CVPR 2016)

<br><b>MS MARCO (Microsoft Machine Reading Comprehension Dataset)</b>
<br>Bajaj et al. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset (EMNLP 2016)

<br><b>Tensorflow</b>
<br>Mart√≠n Abad et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems (2016)

<br><b>OpenAI Gym</b>
<br>Brockman et al. OpenAI Gym (2016)

<br><b>XGBoost</b>
<br>Tianqi Chen et al. XGBoost: A Scalable Tree Boosting System (KDD 2016)

<br><b>Image Style Transfer</b>
<br>Leon A. Gatys et al. Image Style Transfer Using Convolutional Neural Networks (CVPR 2016)

<br><b>Deep speech 2</b>
<br>Amodei, D et al. Deep speech 2: End-to-end speech recognition in english and mandarin. (ICML 2016)

<br><b>Continual Learning</b>
<br>James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks (2016)

<br><b>Random Erasing</b>
<br>Sachin Ravi et al. Optimization as a Model For Few-Shot Learning (2016)
<br>Zhun Zhong et al. Random Erasing Data Augmentation (2017)

<br><b>RetinaNet</b>
<br>Tsung-Yi Lin et al. Focal loss for dense object detection (2017)

<br><b>Mask R-CNN</b>
<br>Kaiming He et al. Mask R-CNN for Object Detection and Segmentation (2017)

<br><b>PPO</b>
<br>Schulman, John, et al. Proximal policy optimization algorithms (2017)

<br><b>NAS(Neural Architecture Search)</b>
<br>Barret Zoph et al. Neural Architecture Search with Reinforcement Learning (ICLR 2017)

<br><b>SHAP(Shapley Additive Explanations)</b>
<br>Scott Lundberg et al. A Unified Approach to Interpreting Model Predictions (NeurIPS 2017)

<br><b>Graph Convolutional Networks(GCN)</b>
<br>Thomas N. Kipf et al. Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017)

<br><b>Image Restoration</b>
<br>Dmitry Ulyanov et al. Deep Image Prior (2017)

<br><b>Open-Domain Question Answering</b>
<br>Danqi Chen et al. Reading Wikipedia to Answer Open-Domain Questions. (ACL 2017)
<br>Karpukhin et al. Dense Passage Retrieval for Open-Domain Question Answering. (EMNLP, 2020)

<br><b>GraphSAGE</b>
<br>William L. Hamilton et al. Inductive Representation Learning on Large Graphs (NerulIPS 2017)

<br><b>Loss Function</b>
<br>Dong Yu et al. Permutation Invariant Training of Deep Models Forspeaker-independent Multi-talker Speech Separation (IEEE 2017)

<br><b>IPlaces</b>
<br>Zhou et al. Places: A 10 Million Image Database for Scene Recognition (IEEE 2017)

<br><b>Meta Learning</b>
<br>Chelsea Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (ICML 2017)

<br><b>Weight & Activation Quantizer</b>
<br>Shuchang Zhou et al. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients (IEEE 2017)

<br><b>Opennmt</b>
<br>Guillaume Klein et al. OpenNMT: Open-Source Toolkit for Neural Machine Translation (ACL 2017)

<br><b>ICARLA (Car Learning to Act)</b>
<br>Dosovitskiy et al. CARLA: An Open Urban Driving Simulator (2017)

<br><b>MobileNet</b>
<br>Andrew G. Howard et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017)

<br><b>VoxCeleb1</b>
<br>Nagrani et al. VoxCeleb: a large-scale speaker identification datasett (Interspeech 2017)

<br><b>Kinetics (Kinetics Human Action Video Dataset)</b>
<br>Kay et al. The Kinetics Human Action Video Dataset (2017)

<br><b>ScanNet</b>
<br>Dai et al. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes (CVPR 2017)

<br><b>AudioSet</b>
<br>Jort F. Gemmeke et al. Audio Set: An ontology and human-labeled dataset for audio events (2017)

<br><b>Fashion-MNIST</b>
<br>Xiao et al. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms (2017)

<br><b>Visual Genome</b>
<br>Krishna et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations (2017)

<br><b>Alphago</b>
<br>Silver, D et al. Mastering the game of Go without human knowledge (2017)

<br><b>Alphazero</b>
<br>David Silver et al.  Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (2017)

<br><b>Text Style Transfer</b>
<br>Tianxiao Shen et al. Style Transfer from Non-Parallel Text by Cross-Alignment (NeurIPS 2017)

<br><b>Transformer</b>
<br>A. Vaswani et al. Attention is all you need (2017)

<br><b>BERT</b>
<br>J. Devlin et al. Bert: Pre-training of deep bidirectional transformers for language understanding (2018)

<br><b>GPT</b>
<br>Alec Radford et al. Improving Language Understanding by Generative Pre-Training (2018)

<br><b>GPT-2</b>
<br>Alec Radford et al. Language Models are Unsupervised Multitask Learners (2018)

<br><b>RoBERTa</b>
<br>Yinhan Liu et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach (2018)

<br><b>CornerNet</b>
<br>Hei Law et al. CornerNet: Detecting Objects as Paired Keypoints (ECCV 2018)

<br><b>AutoEncoder-based Recommendation System</b>
<br>Dawen Liang et al. Variational Autoencoders for Collaborative Filtering. (WWW 2018)

<br><b>VoxCeleb2</b>
<br>Chung et al. VoxCeleb2: Deep Speaker Recognition (ISCA 2018)

<br><b>GLUE</b>
<br>Wang et al. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (EMNLP 2018)

<br><b>GAT</b>
<br>Petar Veliƒçkoviƒá et al. Graph Attention Networks (ICLR 2018)

<br><b>fastMRI</b>
<br>Zbontar et al. fastMRI: An Open Dataset and Benchmarks for Accelerated MRI (2018)

<br><b>Speech Commands</b>
<br>Pete Warden et al. Warden in Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition (2018)

<br><b>MultiNLI (Multi-Genre Natural Language Inference)</b>
<br>Williams et al. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference (NAACL 2018)

<br><b>Session-based Recommendation System</b>
<br>Self-Attentive Sequential Recommendationr (ICDM 2018)

<br><b>BLAS (Basic Linear Algebra Subprograms)</b>
<br>C. Nugteren, CLBlast: A tuned OpenCL BLAS library (2018)

<br><b>Low Distortion & Good Perceptual Quality</b>
<br>Yochai Blau et al. The Perception-Distortion Tradeoff (CVPR 2018)

<br><b>Albumentations</b>
<br>Alexander Buslaev et al. Albumentations: fast and flexible image augmentations (2018)

<br><b>AlphaStar</b>
<br>Google Deepmind. AlphaStar: Mastering the real-time strategy game StarCraft II (2019)

<br><b>EfficientNet</b>
<br>Mingxing Tan et al. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)

<br><b>Backdoor Attack</b>
<br>Tianyu Gu et al. BadNets: Evaluating Backdooring Attacks on Deep Neural Networks (NeurIPS 2019)

<br><b>SentencePiece</b>
<br>Taku Kudo et al. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (ACL 2019)

<br><b>Specaugment</b>
<br>Daniel S. Park et al.  Specaugment: A simple data augmentation method for automatic speech recognition  (Interspeech 2019)

<br><b>EDA</b>
<br>Jason Wei et al. EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks (EMNLP-IJCNLP 2019)

<br><b>Label Smoothing</b>
<br>Rafael M√ºller et al. When Does Label Smoothing Help? (NeurIPS 2019)

<br><b>GIoU Loss</b>
<br>Hamid Rezatofighi et al. Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression (CVPR 2019)

<br><b>AutoAugment</b>
<br>Ekin D. Cubuk et al. AutoAugment: Learning Augmentation Policies from Data (CVPR 2019)

<br><b>Scipy</b>
<br>Pauli Virtanen et al. SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python (2019)

<br><b>Natural Questions</b>
<br>Kwiatkowski et al. Natural Questions: a Benchmark for Question Answering Research (TACL 2019)

<br><b>CoLA (Corpus of Linguistic Acceptability)</b>
<br>Warstadt et al. Neural Network Acceptability Judgments (TACL 2019)

<br><b>Data Augmentation </b>
<br>Jason Wei et al. EDA : Easy Data Augmentation Techniques for Boosting Performance on Text Classification (EMNLP-IJCNLP 2019)

<br><b>SuperGLUE</b>
<br>Wang et al. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems (NeurIPS 2019)

<br><b>Hugging face</b>
<br>Thomas Wolf et al. HuggingFace's Transformers: State-of-the-art Natural Language Processingt (EMNLP 2019)

<br><b>GPU</b>
<br>Jeff Johnson et al. Billion-scale similarity search with GPUs (IEEE 2019)

<br><b>IFFHQ (Flickr-Faces-HQ)</b>
<br>Karras et al. A Style-Based Generator Architecture for Generative Adversarial Networks (CVPR 2019)

<br><b>T5</b>
<br>Colin Raffel et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (The Journal of Machine Learning Research 2019)

<br><b>RAG</b>
<br>Patrick Lewis et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NeurIPS 2020)

<br><b>MoCo</b>
<br>Kaiming He et al. Momentum Contrast for Unsupervised Visual Representation Learning (2019)

<br><b>Robotics: Vision and Touch Representation</b>
<br>Michelle A. Lee et al. Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks (ICRA 2019)

<br><b>MuZero</b>
<br>Julian Schrittwieser et al. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model  (Nature 2020)

<br><b>wav2vec</b>
<br>S. Schneider et al. wav2vec: Unsupervised pre-training for speech recognition (Interspeech 2019)
<br>Alexei Baevski et al. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations Neural Turing Machines (NeurlIPS 2020)

<br><b>GPT-3, Prompt Tuning</b>
<br>Brown et al. Language Models are Few-Shot Learners (NeurIPS 2020)

<br><b>UMAP(Uniform Manifold Approximation and Projection)</b>
<br>Leland McInnes et al. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction (2020)

<br><b>DETR</b>
<br>Nicolas Carion et al. End-to-End Object Detection with Transformers (2020)

<br><b>SimCLR</b>
<br>Chen Ting et al. Simple Framework for Contrastive Learning of Visual Representations (PMLR 2020)
<br>Chen Ting et al. Big self-supervised models are strong semi-supervised learners. (2020)

<br><b>UDA(Unsupervised Data Augmentation)</b>
<br>Qizhe Xie et al. Unsupervised Data Augmentation for Consistency Training (NeurIPS 2020)

<br><b>MLIR (Multi Level Intermediate Representation)</b>
<br>C. Lattner et al. MLIR: A compiler infrastructure for the end of Moore‚Äôs law (2020)

<br><b>GNN-based Recommendation System</b>
<br>Xiangnan He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation (SIGIR 2020)

<br><b>OGB (Open Graph Benchmark)</b>
<br>Hu et al. Open Graph Benchmark: Datasets for Machine Learning on Graphs (NIPS 2020)

<br><b>nuScenes</b>
<br>Caesar et al. nuScenes: A multimodal dataset for autonomous driving (CVPR 2020)

<br><b>CORD-19</b>
<br>Wang et al. CORD-19: The COVID-19 Open Research Dataset (ACL 2020)

<br><b>NeRF(Neural Radiance Fields)</b>
<br>Mildenhall et al. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020)

<br><b>Retrieval-Augmented Generation</b>
<br>Patrick Lewis et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NeurIPS 2020)

<br><b>Vision Transformer(ViT)</b>
<br>Alexey Dosovitskiy Gu et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020)

<br><b>Reinforcement Learning from Human Feedback (RLHF)</b>
<br>Nisan Stiennon et al. Learning to summarize from human feedback (NeurIPS 2020)
<br>Long Ouyang et al. Training language models to follow instructions with human feedback (2022)

<br><b>BART</b>
<br>Mike Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (ACL 2020)

<br><b>Scaling Law</b>
<br>Jared Kaplan et al. Scaling Laws for Neural Language Models (2020)
<br>Tom Henigh et al. Scaling Laws for Autoregressive Generative Modeling  (2020)
<br>Leo Gao et al. Scaling Laws for Reward Model Overoptimization (2022)
<br>Aidan Clark et al. Unified Scaling Laws for Routed Language Models  (2022)
<br>Yi Tay et al. Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers (ICLR 2022)
<br>Jordan Hoffmann et al. Training Compute-Optimal Large Language Models (NeurIPS 2022)
<br>Jason Wei et al. Emergent Abilities of Large Language Models (TMLR 2022)

<br><b>Prefix Tuning</b>
<br>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. (ACL 2021)

<br><b>Generative Artificial Intelligence(GAI)</b>
<br>R Bommasani et al. On the opportunities and risks of foundation models (2021)

<br><b>Swin Transformer</b>
<br>Ze Liu et al. Hierarchical Vision Transformer using Shifted Windows (2021)

<br><b>Reinforcement Learning</b>
<br>David Silver et al. Reward is enough (Artificial Intelligence 2021)

<br><b>Text to Image Generation</b>
<br>Aditya Ramesh et al. DALL-E: Zero-Shot Text-to-Image Generation (JMLR 2021)
<br>Aditya Ramesh et al. Hierarchical Text-Conditional Image Generation with CLIP Latents (2022)
<br>James Betke et al. Improving Image Generation with Better Captions (2023)

<br><b>ROUGE</b>
<br>Chin-Yew Lin et al. ROUGE: A Package for Automatic Evaluation of Summaries (2021)

<br><b>Stable Diffusion</b>
<br>Rombach et al. High-Resolution Image Synthesis with Latent Diffusion Models (2021)

<br><b>AlphaFold</b>
<br>Richard Evans et al. Protein complex prediction with AlphaFold-Multimer. (2021)
<br>John Jumper et al. Highly accurate protein structure prediction with AlphaFold (Nature 2021)
<br>Patrick Bryant et al. Predicting the structure of large protein complexes using AlphaFold and Monte Carlo tree search (Nature Communications 2022)
<br>Josh Abramson et al. Accurate structure prediction of biomolecular interactions with AlphaFold 3 (Nature 2024)

<br><b>AlphaProteo</b>
<br>Vinicius Zambaldi et al. De novo design of high-affinity protein binders with AlphaProteo (2024)

<br><b>IMDb Movie Reviews</b>
<br>Andrew L. Maas et al. Learning Word Vectors for Sentiment Analysis (ACL 2021)

<br><b>Pytorch</b>
<br>Adam Paszk et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library (NeurIPS 2021)

<br><b>Stochastic Parrot</b>
<br>Emily M. Bender et al. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú (2021)

<br><b>Langchain</b>
<br>Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models (ICLR 2022)

<br><b>AlphaChord</b>
<br>Yujia Li et al. Competition-Level Code Generation with AlphaCode (2022)

<br><b>GraphCast</b>
<br>Remi Lam et al. GraphCast: Learning skillful medium-range global weather forecasting  (2022)

<br><b>OPT</b>
<br>Susan Zhang et al. OPT: Open Pre-trained Transformer Language Models (2022)

<br><b>InstructGPT</b>
<br>Long Ouyang et al. Training language models to follow instructions with human feedback (2022)

<br><b>PaLM</b>
<br>Aakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways (2022)

<br><b>Chain-of-Thought Prompting</b>
<br>Jason Wei et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022)

<br><b>Non-language Task</b>
<br>Tuan Dinh et al. LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks (NeurIPS 2022)

<br><b>3D Generator</b>
<br>Eric R. Chan et al. G3D: Efficient Geometry-aware 3D Generative Adversarial Networks (CVPR 2022)

<br><b>Joint Embedding Predictive Architecture(JEPA)</b>
<br>Yann LeCun et al. A Path Towards Autonomous Machine Intelligence (2022)

<br><b>GPT-4</b>
<br>Baolin Peng et al. Instruction Tuning with GPT-4 (EMNLP 2023)

<br><b>LLAMA</b>
<br>Touvron et al. LLaMA: Open and Efficient Foundation Language Models (2023)

<br><b>LLaVa</b>
<br>Haotian Liu et al. Visual Instruction Tuning (NeurIPS 2023)

<br><b>Alpaca, Instruction Tuning</b>
<br>Rohan Taori et al. Alpaca: A Strong, Replicable Instruction-Following Model (2023)

<br><b>Large Language Model (LLM)</b>
<br>Tyna Eloundou et al. GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models (2023)
<br>Brandon C. Roy et al. Predicting the birthe of a spoken word (2015)
<br>Tom McCoy et al. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference (ACL 2019)
<br>Qihuang Zhong et al. Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT (2023)
<br>Lukas Berglund et al. The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A" (2023)

<br><b>Quantization</b>
<br>Tim Dettmers et al. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale (NeurIPS 2022)
<br>Tim Dettmers et al. QLoRA: Efficient Finetuning of Quantized LLMs (NeurIPS 2023)
<br>Hongyu Wang et al. BitNet: Scaling 1-bit Transformers for Large Language Models (2023)
<br>Shuming Ma et al. The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (2024)

<br><b>DPO</b>
<br>Rafael Rafailov et al. Direct Preference Optimization: Your Language Model is Secretly a Reward Model (NeurIPS 2023)

<br><b>Semantic Planner</b>
<br>Ma et al. Eureka: Human-Level Reward Design via Coding Large Language Models (ICLR 2024)

<br><b>AlphaGeometry</b>
<br>Trieu H. Trinh et al. Solving olympiad geometry without human demonstrations (2024)

<br><b>AlphaProof</b>
<br>Google Deepmind. AI achieves silver-medal standard solving International Mathematical Olympiad problems  (2024)

<br><b>Habsburg AI</b>
<br>Ilia Shumailov et al. The Curse of Recursion: Training on Generated Data Makes Models Forget (2023)
<br>Ilia Shumailov et al. AI models collapse when trained on recursively generated data (Nature 2024)

<br><b>Machine Unlearning</b>
<br>Weijia Shi et al. Detecting Pretraining Data from Large Language Models (ICLR 2024)
<br>Martin Pawelczyk et al. In-Context Unlearning: Language Models as Few-Shot Unlearners (ICML 2024)
<br>Michael Duan et al. Do Membership Inference Attacks Work on Large Language Models (COLM 2024)

<br><b>Veo & Imagen</b>
<br>Google Deepmind. State-of-the-art video and image generation with Veo 2 and Imagen 3 (2024)

<br><b>RT-X</b>
<br>Abby O‚ÄôNeill et al. Open X-Embodiment: Robotic Learning Datasets and RT-X Models (2024)

<br><b>Smollm</b>
<br>Allal, L. B et al. Smollm - blazingly fast and remarkably powerful (2024)
<br>Loubna Ben Allal et al. SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model (2025)

<br><b>MobileLLM</b>
<br>Zechun Liu et al. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases  (ICML 2024)

<br><b>The Nobel Prize in Physics 2024</b>
<br>John J. Hopfield, Geoffrey E. Hinton

<br><b>The Nobel Prize in Chemistry 2024</b>
<br>David Baker, Demis Hassabis, John M. Jumper

<br><b>Rule-Based Reward</b>
<br>Yecheng Jason Ma et al. Eureka: Human-Level Reward Design via Coding Large Language Models (ICLR 2024)

<br><b>GRPO(Group Relative Policy Optimization)</b>
<br>Zhihong Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (2024)

<br><b>DeepSeek</b>
<br>DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025)

<br><br><br>


# Reference
[Annotated History of Modern AI and Deep Learning (Juergen Schmidhuber)](https://arxiv.org/abs/2212.11279)
<br>[Classical Paper List on Machine Learning andNatural Language Processing (Zhiyuan Liu)](http://nlp.csai.tsinghua.edu.cn/~lzy/resources/paper_list_ml_nlp.html)
<br>[Award-winning classic papers in ML and NLP (Desh Raj)](https://desh2608.github.io/2018-08-30-classic-papers/)
<br>[Computer Vision: 10 Papers to Start (Chenxi Liu)](https://www.cs.jhu.edu/~cxliu/2015/computer-vision-10-papers-to-start.html)
<br>[Awesome - Most Cited Deep Learning Papers (Terryum)](https://github.com/terryum/awesome-deep-learning-papers)
<br>[Papers With Code Machine Learning Datasets](https://paperswithcode.com/datasets)
<br>[ÏïºÏÇ¨ÏôÄ ÎßåÌôîÎ°ú Î∞∞Ïö∞Îäî Ïù∏Í≥µÏßÄÎä• Í∞ïÏùò](https://brunch.co.kr/magazine/yamanin)
<br>[The Nobel Prize in Physics 2024](https://www.nobelprize.org/prizes/physics/2024/summary/)
<br>[The Nobel Prize in Chemistry 2024](https://www.nobelprize.org/prizes/chemistry/2024/summary/)

<br><br><br>
